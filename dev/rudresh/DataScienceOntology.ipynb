{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36764bitmyenvconda9fe1b81989894e709052b0eae69a3369",
   "display_name": "Python 3.6.7 64-bit ('myenv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "onto = get_ontology(\"/home/rudresh/Documents/MSC3/S5/Code/Data/data.owl\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'classes' property is not defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4ee3e1def74f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.semanticweb.org/rudresh/ontologies/2020/1/dataScience-ontology-8#Deep_Learning'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mIRIS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/owlready2/entity.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(Class, attr)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0mProp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_props\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mProp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'%s' property is not defined.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAnnotationProperty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"__%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;31m# Do NOT cache as such in __dict__, to avoid inheriting annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'classes' property is not defined."
     ]
    }
   ],
   "source": [
    "# onto.search(iri = \"AdaBoost\")\n",
    "temp = 'http://www.semanticweb.org/rudresh/ontologies/2020/1/dataScience-ontology-8#Deep_Learning'\n",
    "type(temp)\n",
    "t =IRIS[temp].classes()\n",
    "print(t)\n",
    "\n",
    "# onto.search_one(label = \"AdaBoost\")\n",
    "\n",
    "# onto.search(iri = \"*Boost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[data.AdaBoost,\n data.Ensemble,\n data.Agnostic_Specifications,\n data.Production,\n data.Apriori_algorithm,\n data.Rule_System,\n data.Arithmetic_Mean,\n data.Statistics,\n data.BI_and_Datawarehousing,\n data.Coreskills,\n data.Back-Propagation,\n data.Neural_Networks,\n data.Bayesian,\n data.Learning_Algorithm,\n data.Boosting,\n data.BootStrap_Sampling,\n data.Model_Validation,\n data.Business_Analysis_and_IT_Projects,\n data.C4.5,\n data.Decision_Tree,\n data.C5.0,\n data.Cloud,\n data.Cluster_Validity,\n data.Clustering,\n data.Coefficient_of_variation,\n data.Conditional_Decision_Trees,\n data.Confusion_Matrix,\n data.DomainConcept,\n data.Cross_Validation,\n data.Data_Cleaning,\n data.Data_Science,\n data.Data_Engineering,\n data.Data_Prepration,\n data.Data_Visualization,\n data.Databases,\n data.Dataflow,\n data.Pradigms,\n data.Decision_Stump,\n data.Deep_Learning,\n data.Deep_belief_network,\n data.Dimensionality_Reduction,\n data.Distributed_ML_Libraries,\n data.Eclat_algorithm,\n data.Elastic_Net,\n data.Regularization,\n data.Gaussian_Naive_Bayes,\n data.Hierarchical_Clustering,\n data.Hive,\n data.Platforms,\n data.Hopfield_Network,\n data.IT_Security_Implementation,\n data.IT_Security_Operations,\n data.IT_Security_Standards,\n data.IT_Support,\n data.Instance_Based,\n data.Intelligence,\n data.Interquartile_Range,\n data.Kappa_Statistics,\n data.Legacy_Mainframe,\n data.Linear_Discriminant_Analysis,\n data.Logistic_Regression,\n data.Linear_Regression,\n data.Regression,\n data.M5,\n data.Mainframe_Programming,\n data.Map_reduce,\n data.Mean,\n data.Mean_Absolute_Deviation,\n data.Mode,\n data.Model_Performance,\n data.Multinomial_Naive_Bayes,\n data.Multiple_Linear_Regression,\n data.Naive_Bayes,\n data.Networks,\n data.Perceptron,\n data.Pig,\n data.Precision_and_Recall,\n data.Programming_Languages,\n data.Projection_Pursuit,\n data.Quadratic_Discriminant_Analysis,\n data.ROC_curves,\n data.Random_Forest,\n data.Resource_Managers,\n data.Ridge_Regression,\n data.Sammon_Mapping,\n data.Sampling_Theory,\n data.Sensitivity_and_Specificity,\n data.Servers_and_Middleware,\n data.Simple_Linear_Regression,\n data.Skewness_of_statistical_data,\n data.SoftSkills,\n data.Software_Development,\n data.Stacked_Auto_Encoders,\n data.Standard_error_of_mean,\n data.Stepwise_Regression,\n data.Stochastic_Gradient_Descent,\n data.System_Administration,\n data.YARN,\n data.adabas,\n data.agile,\n data.amazon_web_services_aws,\n data.analyst,\n data.apache_hadoop,\n data.apache_webserver,\n data.application_support,\n data.architecture,\n data.arcsight,\n data.arcsite,\n data.as_400,\n data.asp,\n data.asset,\n data.avaya,\n data.bash,\n data.basic_internet_skills,\n data.big_data,\n data.broadband,\n data.business_analysis,\n data.business_intelligence,\n data.business_objects,\n data.business_process,\n data.business_requirements_documentation,\n data.business_solutions,\n data.business_systems_analysis,\n data.capacity_management,\n data.certification_accreditation,\n data.chef,\n data.cisco,\n data.cisco_routers,\n data.citrix,\n data.client,\n data.clustering,\n data.cobol,\n data.cognos_impromptu,\n data.collaborate,\n data.computer_Networking,\n data.computer_engineering,\n data.computer_hardware_hardware_knowledge,\n data.computer_installation_and_setup,\n data.configuration_management,\n data.content,\n data.correlation_coefficient,\n data.counter_intelligence,\n data.covariance,\n data.cryptography,\n data.crystal_reports,\n data.customer,\n data.customer_information_control_system_cics,\n data.data_architecture,\n data.data_integration,\n data.data_management,\n data.data_migration,\n data.data_modelling,\n data.data_structures,\n data.data_visualisation,\n data.data_warehousing,\n data.database_administration,\n data.database_design,\n data.db2,\n data.department,\n data.development,\n data.disaster_recovery_planning,\n data.document_management,\n data.domain,\n data.domain_name_system_dns,\n data.dv_clearance,\n data.dv_level,\n data.dynamic_host_configuration_protocol_dhcp,\n data.enterprise_software,\n data.esql,\n data.ethernet,\n data.extensible_markup_language_xml,\n data.extensible_stylesheet_language_xsl,\n data.extraction_transformation_and_loading_etl,\n data.firewalls,\n data.full_life_cycle,\n data.function,\n data.geospatial_intelligence,\n data.global_system_for_mobile_communications_gsm,\n data.hardware_and_software_configuration,\n data.help_desk_support,\n data.hibernate,\n data.human_intelligence,\n data.hyper-v,\n data.ibm_mq_series,\n data.ibm_resource_access_control_facility_racf,\n data.ibm_websphere,\n data.icl_vme,\n data.imagery_intelligence,\n data.informatica,\n data.information_architecture,\n data.information_assurance,\n data.information_security,\n data.information_systems,\n data.infrastructure,\n data.initiative,\n data.integrated_development_environment_ide,\n data.intelligence_analysis,\n data.interest,\n data.internet_hosting,\n data.internet_service_provider_isp,\n data.internet_services,\n data.iso_iec_27001,\n data.it_management,\n data.it_recruiting,\n data.it_strategy,\n data.it_support,\n data.itil,\n data.java,\n data.java_message_service_jms,\n data.java_server_pages_jsp,\n data.jboss,\n data.jdbc,\n data.job_control_language,\n data.job_control_language_jcl,\n data.junit,\n data.k-Means,\n data.k-Medians,\n data.kanban,\n data.lansa,\n data.lifecycle_management,\n data.linc,\n data.linux,\n data.lotus_notes,\n data.macintosh_os,\n data.mainframe,\n data.mainframe_systems,\n data.management,\n data.median,\n data.member,\n data.methodology,\n data.microfocus,\n data.microsoft_access,\n data.microsoft_c,\n data.microsoft_crm,\n data.microsoft_dynamics,\n data.microsoft_exchange,\n data.microsoft_networking,\n data.microsoft_operating_systems,\n data.microsoft_powershell,\n data.microsoft_sharepoint,\n data.microsoft_sql,\n data.microsoft_sql_server_integration_services_ssis,\n data.microsoft_visio,\n data.microsoft_vista,\n data.middleware,\n data.mongodb,\n data.mqseries,\n data.mvs,\n data.network_administration,\n data.network_attached_storage_nas,\n data.network_engineering,\n data.network_hardware,\n data.network_hardware_software_maintenance,\n data.network_installation,\n data.network_security,\n data.network_support,\n data.nosql,\n data.object_orientated_analysis_and_design_ooad,\n data.online_analytical_processing_olap,\n data.optimisation,\n data.oracle,\n data.oracle_business_intelligence_en-terprise_edition_obiee,\n data.oracle_pl_sql,\n data.os_400,\n data.ospf,\n data.overview,\n data.partner,\n data.perl,\n data.phone_systems,\n data.postgresql,\n data.powerbuilder,\n data.practice,\n data.prince2,\n data.printers,\n data.pro_c,\n data.problem_analysis,\n data.process_testing,\n data.product,\n data.program,\n data.project_planning_and_development_skills,\n data.puppet,\n data.python,\n data.qlikview,\n data.quick_test_professional_qtp,\n data.raid,\n data.rapid_application_development,\n data.red_hat_linux,\n data.relational_database_management_system_rdbms,\n data.relational_databases,\n data.requirements_analysis,\n data.resource,\n data.responding_to_technical_questions,\n data.rexx,\n data.role,\n data.rpg,\n data.rpg400,\n data.rpg_iv,\n data.ruby,\n data.scaling,\n data.scrum,\n data.security_implementation,\n data.servlets,\n data.shell_scripts,\n data.signals_intelligence,\n data.simple_moving_average,\n data.soap,\n data.software_Testing,\n data.software_architecture,\n data.software_as_a_service_saas,\n data.software_development,\n data.software_engineering,\n data.software_installation,\n data.software_use_instruction,\n data.solaris,\n data.solution_architecture,\n data.spring_framework,\n data.sql,\n data.sql_server,\n data.sql_server_analysis_services_ssas,\n data.sql_server_reporting_services_ssrs,\n data.ssl,\n data.staff_augmentation,\n data.structured_programming,\n data.struts,\n data.switches,\n data.sybase,\n data.symantec_packages,\n data.system_administration,\n data.system_and_network_configuration,\n data.system_architecture,\n data.systems_administration,\n data.systems_analysis,\n data.systems_development,\n data.systems_development_life_cycle_sdlc,\n data.systems_integration,\n data.systems_management,\n data.t-test,\n data.technical_writing_editing,\n data.test_director,\n data.tomcat,\n data.trading_systems,\n data.transact-sql,\n data.transmission_control_protocol_internet_protocol_tcp_ip,\n data.troubleshooting_technical_issues,\n data.uniface,\n data.unified_modelling_language_uml,\n data.unix,\n data.unix_shell_scripting,\n data.user_acceptance_testing_uat,\n data.vbscript,\n data.version_control,\n data.video_conferencing,\n data.virtual_private_networking_vpn,\n data.virtual_storage_access_method_vsam,\n data.visual_basic,\n data.visual_studio,\n data.vmware_esxi,\n data.web_servers,\n data.weblogic,\n data.wide_area_network_wan,\n data.windows_server,\n data.winrunner,\n data.workstations,\n data..net_programming,\n data.3g,\n data.Architectures/Frameworks,\n data.Averaged_One-Dependence_Estimators_(AODE),\n data.Bayesian_Belief_Network_(BBN),\n data.Bayesian_Network_(BN),\n data.Bessel’s_Interpolation,\n data.Bootstrapped_Aggregation_(Bagging),\n data.Chi-squared_Automatic_Interaction_Detection_(CHAID),\n data.Classification_and_Regression_Tree_(CART),\n data.Convolutional_Neural_Network_(CNN),\n data.Deep_Boltzmann_Machine_(DBM),\n data.Expectation_Maximisation_(EM),\n data.Flexible_Discriminant_Analysis_(FDA),\n data.Gradient_Boositng_Machines(GBM),\n data.Gradient_Boosted_Regression_Trees(CBRT),\n data.Iterative_Dichotomiser_3_(ID3),\n data.Learning_Vector_Quantization_(LVQ),\n data.Least-Angle_Regression_(LARS),\n data.Least_Absolute_Shrinkage_and_Selection_Operator_(LASSO),\n data.Linear_Discriminant_Analysis_(LDA),\n data.Locally_Estimated_Scatterplot_Smoothing_(LOESS),\n data.Locally_Weighted_Learning_(LWL),\n data.Long_Short-Term_Memory_Networks_(LSTMs),\n data.Mixture_Discriminant_Analysis_(MDA),\n data.Multidimensional_Scaling_(MDS),\n data.Multilayer_Perceptrons_(MLP),\n data.Multivariate_Adaptive_Regression_Splines_(MARS),\n data.Ordinary_Least_Squares_Regression_(OLSR),\n data.Partial_Least_Squares_Regression_(PLSR),\n data.Principal_Component_Analysis_(PCA),\n data.Principal_Component_Regression_(PCR),\n data.Quadratic_Discriminant_Analysis_(QDA),\n data.Radial_Basis_Function_Network_(RBFN),\n data.Recurrent_Neural_Networks_(RNNs),\n data.Self-Organizing_Map_(SOM),\n data.Spearman’s_Rank_Correlation,\n data.Stacked_Generalization(Blending),\n data.Stacked_Generalization_(Stacking),\n data.Support_Vector_Machines_(SVM),\n data.Weighted_Average_(Blending),\n data.c++,\n data.k-Nearest_Neighbor_(kNN)]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(onto.classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import RegexpParser, tree\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = \"DBW_CONCEPT: {<JJ.*>*<HYPH>*<JJ.*>*<HYPH>*<NN.*>*<HYPH>*<NN.*>+}\" #good for syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = {\n",
    "        \"abstract\": \"AI talent acquisition platform: Designed, created and deployed a framework using Conditional random field and Stanford core NLP model to parse the resume and job positions for a military organisation NATO.◦ IBM Cloud: Created and deployed docker and cloud foundry application on IBM Bluemix. Graph QL: Developed a GraphQL API to respond for the queries which predicts the estimated salary from the model based on location ,industry and job title inputs. Graph Database: Designed, deployed a Neo4j graph database script to migrate data from MySQL To Neo4j. Web Crawler: Created a python crawler script to scrape the data from the HTML pages using beautiful soup. ◦ Text Analytic: Created a rule based model to work as an extractor that pulls structured information from unstructured or semi-structured text using Annotated Query Language.Data Analyst: Used different statistical technique to gather data insights and presented in depth findings to improve business strategies and operations. ◦ Oracle Database Administration: Scheduled the database replication script to sync the data. Performed the database refresh, patch and upgrade on Oracle 11gR1/11gR2. Analyzed the database performance. Created partition and indexes on tables to improve the performance. Fine-tuned SQL queries to improve the efficiency. Data Migration: Created the PL/SQL stored procedures,functions and packages to migrate data from oracle db to Maria db using oracle JDBC. Unix Shell Scripting: Created a unix shell script to automate the daily activity task as per client requirement and deployed it on Cron server. Team Management: Managed a team of 4 people by assigning, guiding the task and making deliveries to client. Sentiment Analysis: Sentiment analysis on the amazon reviews using 1D convolution neural network. • Ship Detection in Satellite imagery: Ship detection in optical satellite images using Resnet-Unet model. • Fake reviews detection: Fake review detection on online platform based on user behaviors and text reviews.• Job post and resume matching : Applying the concept of NLP, ML and Multiple criteria sorting method to match and score the resume with job posting. it also involves scraping job post, building taxonomy and ontology.\",\n",
    "        \"keywords\": \"data mining, computer science\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper:\n",
    "    \"\"\" A simple abstraction layer for working on the paper object\"\"\"\n",
    "    \n",
    "    def __init__(self, paper = None, modules = None):\n",
    "        \"\"\" Initialising the ontology class\n",
    "        \"\"\"\n",
    "        self.title = None\n",
    "        self.abstract = None\n",
    "        self.keywords = None\n",
    "        self._text = None\n",
    "        self.chunks = None\n",
    "        self.text_attr = ('title', 'abstract', 'keywords')\n",
    "        self.tagger = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "        \n",
    "        if modules is not None:\n",
    "            self.modules = modules\n",
    "        \n",
    "        if paper is not None:\n",
    "            self.set_paper(paper)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def set_paper(self, paper):\n",
    "        \"\"\"Function that initializes the paper variable in the class.\n",
    "        Args:\n",
    "            paper (either string or dictionary): The paper to analyse. It can be a full string in which the content\n",
    "            is already merged or a dictionary  {\"title\": \"\",\"abstract\": \"\",\"keywords\": \"\"}.\n",
    "        \"\"\"\n",
    "        self.title = None\n",
    "        self.abstract = None\n",
    "        self.keywords = None\n",
    "        self._text = None\n",
    "        self.semantic_chunks = None\n",
    "        self.syntactic_chunks = None\n",
    "        \n",
    "        try:\n",
    "            if isinstance(paper, dict):              \n",
    "                for attr in self.text_attr:\n",
    "                    try: \n",
    "                        setattr(self, attr, paper[attr])\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    \n",
    "                self.treat_keywords()\n",
    "                self.text()\n",
    "                \n",
    "            elif isinstance(paper, str):\n",
    "                self._text = paper.strip()\n",
    "            \n",
    "            else:\n",
    "                raise TypeError(\"Error: Unrecognised paper format\")\n",
    "                return\n",
    "        \n",
    "            self.pre_process()\n",
    "            \n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def text(self):\n",
    "        \"\"\" Text aggregator\n",
    "        \"\"\"\n",
    "        attr_text = [getattr(self, attr) for attr in self.text_attr]\n",
    "        self._text = '. '.join((s.rstrip('.') for s in attr_text if s is not None))\n",
    "\n",
    "\n",
    "    def treat_keywords(self):\n",
    "        \"\"\" Function that handles different version of keyword field\n",
    "        \"\"\"\n",
    "        if self.keywords is None:\n",
    "            return\n",
    "        if isinstance(self.keywords, list):\n",
    "            self.keywords = ', '.join(self.keywords)\n",
    "\n",
    "\n",
    "    def part_of_speech_tagger(self, doc):\n",
    "        \"\"\" Part of speech tagger\n",
    "        Returns:\n",
    "            text (string): single token\n",
    "            tag_ (string): POS tag\n",
    "        \"\"\"\n",
    "        for token in doc:\n",
    "            if token.tag_:\n",
    "                yield token.text, token.tag_\n",
    "                \n",
    "    def remove_root_verb(self, doc):\n",
    "        \"\"\" Creates a string in which it removes verbs that are also root of the tree\n",
    "        \"\"\"   \n",
    "        new_document = doc.text        \n",
    "        items_to_remove = [(token.text,token.dep_,token.pos_,token.idx, token.idx+len(token.text)) for token in doc if (token.pos_ == \"VERB\" and token.dep_ == \"ROOT\")]\n",
    "        for item in reversed(items_to_remove):\n",
    "            new_document = new_document[:item[3]] + \".\" + new_document[item[4]:]\n",
    "        return new_document\n",
    "            \n",
    "\n",
    "    def extraxt_semantic_chuncks(self, pos_tags):\n",
    "        \"\"\" Extract chunks of text from the paper taking advantage of the parts of speech previously extracted.\n",
    "        It uses a grammar\n",
    "        Returns:\n",
    "            chunks (list): list of all chunks of text \n",
    "        \"\"\"\n",
    "        grammar_parser = RegexpParser(GRAMMAR)\n",
    "        chunks = list()\n",
    "        pos_tags_with_grammar = grammar_parser.parse(pos_tags)\n",
    "        #print(pos_tags_with_grammar)\n",
    "        for node in pos_tags_with_grammar:\n",
    "            if isinstance(node, tree.Tree) and node.label() == 'DBW_CONCEPT': # if matches our grammar \n",
    "                chunk = ''\n",
    "                for leaf in node.leaves():\n",
    "                    concept_chunk = leaf[0]\n",
    "                    concept_chunk = re.sub('[\\=\\,\\…\\’\\'\\+\\-\\–\\“\\”\\\"\\/\\‘\\[\\]\\®\\™\\%]', ' ', concept_chunk)\n",
    "                    concept_chunk = re.sub('\\.$|^\\.', '', concept_chunk)\n",
    "                    concept_chunk = concept_chunk.lower().strip()\n",
    "                    chunk += ' ' + concept_chunk\n",
    "                chunk = re.sub('\\.+', '.', chunk)\n",
    "                chunk = re.sub('\\s+', ' ', chunk)\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    \n",
    "    def extraxt_syntactic_chuncks(self, document):\n",
    "        \"\"\" Extract chunks of text from the paper, using stopwords as delimiter.\n",
    "        It uses a grammar\n",
    "        Returns:\n",
    "            chunks (list): list of all chunks of text \n",
    "        \"\"\"\n",
    "        tokenizer = RegexpTokenizer(r'[\\w\\-\\(\\)]*')\n",
    "        tokens = tokenizer.tokenize(document)\n",
    "        filtered_words = [a for a in [w if w not in stopwords.words('english') else ':delimiter:' for w in tokens] if a != '']\n",
    "        matrix_of_tokens = [list(g) for k,g in itertools.groupby(filtered_words,lambda x: x == ':delimiter:') if not k]\n",
    "        return [\" \".join(row).lower() for row in matrix_of_tokens]\n",
    "    \n",
    "                \n",
    "    def pre_process(self):\n",
    "        \"\"\" Pre-processes the paper: identifies the parts of speech and then extracts chunks using a grammar\n",
    "        \"\"\"\n",
    "        ##################### Tagger with spaCy.io\n",
    "        doc = self.tagger(self._text)\n",
    "        \n",
    "        # =============================================================================\n",
    "        #         SYNTACTIC\n",
    "        # =============================================================================\n",
    "        if self.modules == 'syntactic' or self.modules == 'both':\n",
    "            ##################### Getting new filtered document removing ROOT NODES(that are VERBS)\n",
    "            new_filtered_document = self.remove_root_verb(doc)\n",
    "            ##################### Extraxting chunks of text base on stop-words\n",
    "            self.syntactic_chunks = self.extraxt_syntactic_chuncks(new_filtered_document)\n",
    "        # =============================================================================\n",
    "        #         SEMANTIC\n",
    "        # =============================================================================\n",
    "        if self.modules == 'semantic' or self.modules == 'both':\n",
    "            ##################### Getting text and POS in the right configuration\n",
    "            pos_tags = self.part_of_speech_tagger(doc)\n",
    "            ##################### Applying grammar          \n",
    "            self.semantic_chunks = self.extraxt_semantic_chuncks(list(pos_tags))  \n",
    "     \n",
    "        \n",
    "\n",
    "    \n",
    "    def get_text(self):\n",
    "        \"\"\"Returns the text of the paper\n",
    "        \"\"\"\n",
    "        return self._text\n",
    "    \n",
    "    def get_new_text(self):\n",
    "        \"\"\"Returns the text of the paper\n",
    "        \"\"\"\n",
    "        return self.new_text\n",
    "    \n",
    "    def get_semantic_chunks(self):\n",
    "        \"\"\"Returns the chunks extracted from the paper (used by the semantic module)\n",
    "        \"\"\"\n",
    "        return self.semantic_chunks\n",
    "    \n",
    "    def get_syntactic_chunks(self):\n",
    "        \"\"\"Returns the chunks extracted from the paper (used by the syntactic module)\n",
    "        \"\"\"\n",
    "        return self.syntactic_chunks\n",
    "    \n",
    "    def set_modules(self, modules):\n",
    "        \"\"\"Setter for the modules variable\"\"\"\n",
    "        self.modules = modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[' talent acquisition platform', ' framework', ' conditional random field', ' stanford core nlp model', ' resume', ' job positions', ' military organisation nato', ' ◦ ibm cloud', ' docker', ' foundry application', ' ibm bluemix', ' graph ql', ' graphql api', ' queries', ' salary', ' model', ' location', ' industry', ' job title inputs', ' graph database', ' neo4j graph database script', ' data', ' mysql', ' web crawler', ' python crawler script', ' data', ' html pages', ' beautiful soup', ' ◦ text analytic', ' rule', ' model', ' extractor', ' structured information', ' semi structured text', ' annotated query language', ' data analyst', ' different statistical technique', ' data insights', ' depth findings', ' business strategies', ' operations', ' ◦ oracle database administration', ' database replication script', ' data', ' database refresh', ' patch', ' oracle', ' database performance', ' partition', ' indexes', ' tables', ' performance', ' sql', ' efficiency', ' data migration', ' pl', ' sql', ' procedures', ' functions', ' packages', ' data', ' oracle db', ' maria db', ' oracle jdbc', ' unix shell scripting', ' unix shell script', ' daily activity task', ' client requirement', ' cron server', ' team management', ' team', ' people', ' task', ' deliveries', ' client', ' sentiment analysis', ' sentiment analysis', ' amazon reviews', ' 1d convolution', ' neural network', ' ship detection', ' satellite imagery', ' ship detection', ' optical satellite images', ' resnet unet model', ' detection', ' fake review detection', ' online platform', ' user behaviors', ' text', ' job post', ' concept', ' nlp', ' ml', ' multiple criteria', ' method', ' resume', ' job', ' job post', ' taxonomy', ' ontology', ' data mining', ' computer science']\n"
    }
   ],
   "source": [
    "smallPaper = Paper(paper, 'semantic')\n",
    "\n",
    "smallPaper.set_paper(paper)\n",
    "smallPaper.pre_process()\n",
    "print(smallPaper.get_semantic_chunks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import everygrams\n",
    "from kneed import KneeLocator\n",
    "\n",
    "import warnings\n",
    "\n",
    "class Semantic:\n",
    "    \"\"\" A simple abstraction layer for using the Semantic module of the CSO classifier \"\"\"\n",
    "    \n",
    "    def __init__(self, model = None, cso = None, paper = None):\n",
    "        \"\"\"Function that initialises an object of class CSOClassifierSemantic and all its members.\n",
    "        Args:\n",
    "            model (dictionary): word2vec model.\n",
    "            cso (dictionary): Computer Science Ontology\n",
    "            paper (dictionary): paper{\"title\":\"...\",\"abstract\":\"...\",\"keywords\":\"...\"} the paper.\n",
    "        \"\"\"\n",
    "        self.cso = cso                  #Stores the CSO Ontology\n",
    "        self.paper = paper              #Paper to analyse\n",
    "        self.model = model              #contains the cached model          \n",
    "        self.min_similarity = 0.94      #Initialises the min_similarity\n",
    "        self.explanation = dict()\n",
    "        \n",
    "        \n",
    "    def set_paper(self, paper):\n",
    "        \"\"\"Function that initializes the paper variable in the class.\n",
    "        Args:\n",
    "            paper (either string or dictionary): The paper to analyse. It can be a full string in which the content\n",
    "            is already merged or a dictionary  {\"title\": \"\",\"abstract\": \"\",\"keywords\": \"\"}.\n",
    "        \"\"\"\n",
    "        self.paper = paper\n",
    "        self.reset_explanation()\n",
    "        \n",
    "    \n",
    "    def set_min_similarity(self, min_similarity):\n",
    "        \"\"\"Function that initializes the minimum similarity variable.\n",
    "        Args:\n",
    "            min_similarity (float): value of min_similarity between 0 and 1.\n",
    "        \"\"\"\n",
    "        self.min_similarity = min_similarity\n",
    "        \n",
    "        \n",
    "    def reset_explanation(self):\n",
    "        \"\"\" Resetting the explanation \n",
    "        \"\"\"\n",
    "        self.explanation = dict()\n",
    "        \n",
    "        \n",
    "    def get_explanation(self):\n",
    "        \"\"\" Returns the explanation \n",
    "        \"\"\"\n",
    "        return self.explanation\n",
    " \n",
    "    \n",
    "    def classify_semantic(self):\n",
    "        \"\"\"Function that classifies the paper on a semantic level. This semantic module follows four steps: \n",
    "            (i) entity extraction, \n",
    "            (ii) CSO concept identification, \n",
    "            (iii) concept ranking, and \n",
    "            (iv) concept selection.\n",
    "        Args:\n",
    "            processed_embeddings (dictionary): This dictionary saves the matches between word embeddings and terms in CSO. It is useful when processing in batch mode.\n",
    "        Returns:\n",
    "            final_topics (list): list of identified topics.\n",
    "        \"\"\"     \n",
    "\n",
    "        ##################### Core analysis\n",
    "        found_topics, explanation = self.find_topics(self.paper.get_semantic_chunks())\n",
    "    \n",
    "        ##################### Ranking\n",
    "        final_topics = self.rank_topics(found_topics, explanation) \n",
    "             \n",
    "        return final_topics\n",
    "\n",
    "\n",
    "    def find_topics(self, concepts):\n",
    "        \"\"\"Function that identifies topics starting from the ngram forund in the paper\n",
    "        Args:\n",
    "            concepts (list): Chuncks of text to analyse.\n",
    "        Returns:\n",
    "            found_topics (dict): cdictionary containing the identified topics.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set up\n",
    "        found_topics = dict() # to store the matched topics\n",
    "        explanation = dict()\n",
    "\n",
    "        # finding matches\n",
    "        for concept in concepts:\n",
    "            evgrams = everygrams(concept.split(), 1, 3) # list of unigrams, bigrams, trigrams\n",
    "            for grams in evgrams:\n",
    "                gram = \"_\".join(grams)\n",
    "                gram_without_underscore = \" \".join(grams)\n",
    "                #### Finding similar words contained in the model\n",
    "                \n",
    "                list_of_matched_topics = []\n",
    "\n",
    "                if self.model.check_word_in_model(gram):\n",
    "                    list_of_matched_topics = self.model.get_words_from_model(gram)\n",
    "                    \n",
    "                else:                    \n",
    "                    list_of_matched_topics = self.match_ngram(grams)\n",
    "                                    \n",
    "                            \n",
    "                for topic_item in list_of_matched_topics:\n",
    "                        \n",
    "                    topic = topic_item[\"topic\"]\n",
    "                    m     = topic_item[\"sim_t\"]\n",
    "                    wet   = topic_item[\"wet\"]\n",
    "                    sim   = topic_item[\"sim_w\"]\n",
    "                    \n",
    "                    \n",
    "                    if m >= self.min_similarity and topic in self.cso.topics_wu:\n",
    "                        \n",
    "    \n",
    "                        if topic in found_topics:\n",
    "                            #tracking this match\n",
    "                            found_topics[topic][\"times\"] += 1\n",
    "                            \n",
    "                            found_topics[topic][\"gram_similarity\"].append(sim)\n",
    "    \n",
    "                            #tracking the matched gram\n",
    "                            if gram in found_topics[topic][\"grams\"]: \n",
    "                                found_topics[topic][\"grams\"][gram] += 1\n",
    "                            else:\n",
    "                                found_topics[topic][\"grams\"][gram] = 1\n",
    "    \n",
    "                            #tracking the most similar gram to the topic\n",
    "                            if m > found_topics[topic][\"embedding_similarity\"]:\n",
    "                                found_topics[topic][\"embedding_similarity\"] = m \n",
    "                                found_topics[topic][\"embedding_matched\"] = wet\n",
    "    \n",
    "                        else:\n",
    "                            #creating new topic in the result set\n",
    "                            found_topics[topic] = {'grams': {gram:1},\n",
    "                                                    'embedding_matched': wet,\n",
    "                                                    'embedding_similarity': m,\n",
    "                                                    'gram_similarity':[sim],\n",
    "                                                    'times': 1, \n",
    "                                                    'topic':topic}\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        if sim == 1:\n",
    "                            found_topics[topic][\"syntactic\"] = True\n",
    "    \n",
    "                            \n",
    "                            \n",
    "                        primary_label_topic = self.cso.get_primary_label_wu(topic)\n",
    "                        if primary_label_topic not in explanation:\n",
    "                            explanation[primary_label_topic] = set()\n",
    "                        \n",
    "                        explanation[primary_label_topic].add(gram_without_underscore)\n",
    "        \n",
    "        return found_topics, explanation\n",
    "    \n",
    "    \n",
    "    def match_ngram(self, grams, merge=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grams (list): list of tokens to be analysed and founf in the model\n",
    "            merge (boolean): #Allows to combine the topics of mutiple tokens, when analysing 2-grams or 3-grams\n",
    "        \n",
    "        Returns:\n",
    "            list_of_matched_topics (list): containing of all found topics \n",
    "        \"\"\"\n",
    "        \n",
    "        list_of_matched_topics = list()\n",
    "        if len(grams) > 1 and merge:\n",
    "            \n",
    "            temp_list_of_matches = {}\n",
    "            \n",
    "            list_of_merged_topics = {}\n",
    "            \n",
    "            for gram in grams:\n",
    "                if self.model.check_word_in_model(gram):\n",
    "                    list_of_matched_topics_t = self.model.get_words_from_model(gram)\n",
    "                    for topic_item in list_of_matched_topics_t:\n",
    "                        temp_list_of_matches[topic_item[\"topic\"]] = topic_item\n",
    "                        try:\n",
    "                            list_of_merged_topics[topic_item[\"topic\"]] += 1\n",
    "                        except KeyError:\n",
    "                            list_of_merged_topics[topic_item[\"topic\"]] = 1\n",
    "                        \n",
    "            for topic_x, value in list_of_merged_topics.items():\n",
    "                if value >= len(grams):\n",
    "                    list_of_matched_topics.append(temp_list_of_matches[topic_x])\n",
    "        \n",
    "        return list_of_matched_topics\n",
    "                    \n",
    "        \n",
    "    def rank_topics(self, found_topics, explanation):\n",
    "        \"\"\" Function that ranks the list of found topics. It also cleans the explanation accordingly\n",
    "        \n",
    "        Args:\n",
    "            found_topics (dictionary): contains all information about the found topics\n",
    "            explanation (dictionary): contains information about the explanation of topics\n",
    "        \n",
    "        Returns:\n",
    "            final_topics (list): list of final topics\n",
    "        \"\"\"\n",
    "        max_value = 0\n",
    "        scores = []\n",
    "        for tp,topic in found_topics.items(): \n",
    "            topic[\"score\"] = topic[\"times\"] * len(topic['grams'].keys())\n",
    "            scores.append(topic[\"score\"])\n",
    "            if topic[\"score\"] > max_value:\n",
    "                max_value = topic[\"score\"]\n",
    "\n",
    "        for tp,topic in found_topics.items():            \n",
    "            if \"syntactic\" in topic:\n",
    "                topic[\"score\"] = max_value\n",
    "                \n",
    "                \n",
    "             \n",
    "            \n",
    "        # Selection of unique topics  \n",
    "        unique_topics = {}\n",
    "        for tp,topic in found_topics.items():\n",
    "            prim_label = self.cso.get_primary_label_wu(tp)\n",
    "            if prim_label in unique_topics:\n",
    "                if unique_topics[prim_label] < topic[\"score\"]:\n",
    "                    unique_topics[prim_label] = topic[\"score\"]\n",
    "            else:\n",
    "                unique_topics[prim_label] = topic[\"score\"]\n",
    "        \n",
    "        # ranking topics by their score. High-scored topics go on top\n",
    "        sort_t = sorted(unique_topics.items(), key=lambda v: v[1], reverse=True)\n",
    "        #sort_t = sorted(found_topics.items(), key=lambda k: k[1]['score'], reverse=True)\n",
    "        \n",
    "        \n",
    "        # perform \n",
    "        vals = []\n",
    "        for tp in sort_t:\n",
    "            vals.append(tp[1]) #in 0, there is the topic, in 1 there is the info\n",
    "        \n",
    "        \n",
    "        #### suppressing some warnings that can be raised by the kneed library\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        try:\n",
    "            x = range(1,len(vals)+1) \n",
    "            kn = KneeLocator(x, vals, direction='decreasing')\n",
    "            if kn.knee is None:\n",
    "                #print(\"I performed a different identification of knee\")\n",
    "                kn = KneeLocator(x, vals, curve='convex', direction='decreasing')\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        ##################### Pruning\n",
    "        \n",
    "        try: \n",
    "            knee = int(kn.knee)\n",
    "        except TypeError:\n",
    "            knee = 0\n",
    "        except UnboundLocalError:\n",
    "            knee = 0\n",
    "            \n",
    "        if knee > 5:\n",
    "            try:\n",
    "                knee += 0\n",
    "            except TypeError:\n",
    "                print(\"ERROR: \",kn.knee,\" \",knee, \" \", len(sort_t))\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                if sort_t[0][1] == sort_t[4][1]:\n",
    "                    top = sort_t[0][1]\n",
    "                    test_topics = [item[1] for item in sort_t if item[1]==top] \n",
    "                    knee = len(test_topics)\n",
    "    \n",
    "                else:\n",
    "                    knee = 5\n",
    "            except IndexError:\n",
    "                knee = len(sort_t)\n",
    "\n",
    "        final_topics = []\n",
    "        final_topics = [self.cso.get_topic_wu(sort_t[i][0]) for i in range(0,knee)] \n",
    "        self.reset_explanation()\n",
    "        self.explanation = {self.cso.topics_wu[sort_t[i][0]]: explanation[sort_t[i][0]] for i in range(0,knee)}\n",
    "\n",
    "        return final_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_topic = Semantic()"
   ]
  }
 ]
}