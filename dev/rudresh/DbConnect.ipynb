{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36764bitmyenvconda9fe1b81989894e709052b0eae69a3369",
   "display_name": "Python 3.6.7 64-bit ('myenv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['db', 'test', 'admin', 'local']\n['jobs_sovren', 'resumes_sovren', 'experience', 'resumes', 'education']\n"
    }
   ],
   "source": [
    "client = MongoClient('mongodb+srv://user_imt:2020@s5resumesdb-ppukj.azure.mongodb.net/test')\n",
    "print(client.list_database_names())\n",
    "db = client['db']\n",
    "print(db.list_collection_names())\n",
    "db_resumes = db['resumes']\n",
    "one = db_resumes.find_one({ '_id': ObjectId('5e64cbef837ba015d90abc76') })\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "db_jobSoveren = db['jobs_sovren']\n",
    "recent_jobPost = db_jobSoveren.find_one({ '_id': ObjectId('5e64cbef837ba015d90abc76') })\n",
    "r = recent_jobPost['SovrenData']['SkillsTaxonomyOutput'][0]['Taxonomy']\n",
    "informationTechnology = r[0]\n",
    "engineering = r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_resumeSoveren = db['resumes_sovren']\n",
    "recent_resumePost = db_resumeSoveren.find_one({ '_id': ObjectId('5e60f5895a90883323e38bc1') })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Computers'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "school_name = recent_resumePost['Resume']['StructuredXMLResume']['EducationHistory']['SchoolOrInstitution'][1]['School']\n",
    "\n",
    "postal_adress = recent_resumePost['Resume']['StructuredXMLResume']['EducationHistory']['SchoolOrInstitution'][1]['PostalAddress']\n",
    "\n",
    "recent_resumePost['Resume']['StructuredXMLResume']['EducationHistory']['SchoolOrInstitution'][1]['Degree'][0]['DegreeMajor'][0]['Name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"ABOUT TINYCLUES\\r\\n\\r\\n\\r\\nTinyclues’ AI-first marketing platform is built with the latest technology including Kubernetes, Docker, Presto, Python ML frameworks, Elasticsearch or React. Our software stack is a mix between Python and Scala, natively designed for cloud and deployed on AWS. We focus on using those ecosystems for what they do best.\\r\\n\\r\\nThe solution processes and analyzes hundreds of terabytes of data every day from our 100+ enterprise clients across 13 countries. It runs dozens of powerful and carefully designed machine & deep learning algorithms to find the “tiny clues” in our clients’ first-party databases. The technology currently relies on proven datastores and distributed backends to process data. Our data and predictive pipelines are implemented over Kubernetes and orchestrated with an event-driven architecture over RabbitMQ.\\r\\n\\r\\nWe believe empowering people is the most efficient way to build the best product together. The teams are organized in small & autonomous groups working on different business needs with agile methodologies.\\r\\n\\r\\nOur innovative technology has led Tinyclues to be identified by leading IT analyst firm Gartner as a Vendor to Watch for digital marketing analytics and a Cool Vendor in multichannel marketing.\\r\\n\\r\\nBy joining Tinyclues and our 100 team members, you will have an impact on one of top fast-growing start-ups with a unique AI-first vision, breakthrough predictive technology and proven global success.\\r\\n\\r\\nWHAT YOU WILL BE WORKING ON\\r\\nDeep Learning for Recommendation Systems: How to combine implicit & explicit feedback in a deep learning framework (factorizations, higher level interactions, feature transfer, ...)\\r\\nAttention Pooling Networks: How to aggregate a customer's sequence of historical events into a single meaningful vector\\r\\nUnsupervised Learning: How to represent best a customer's socio-demographical information in order to feed our models\\r\\nMeta Learning: How to tune automatically the models' hyper parameters (regularization, latent dimensions, ...) to find the optimal ones\\r\\nPlanning optimization: How to plan best notifications to a user across different subjects, days and communications channels\\r\\nRequirements\\r\\n\\r\\nMust have:\\r\\nknowledge in applied mathematics (probability, gradient descent optimization...) and algorithmic\\r\\nfluency in Python with experience in math libraries (numpy, scipy, sklearn, ...)\\r\\nknowledge of basic machine learning methods (linear regressions, naive bayes, PCA, kmeans)\\r\\nknowledge of object and/or functional programming concepts\\r\\ninternship convention with a French Grande Ecole or University\\r\\nNice to have:\\r\\nknowledge and experience with one of the Neural Network backend (Keras, MXNet, PyTorch)\\r\\nunderstanding the core of the ML libraries\\r\\nrelation database & SQL basics\\r\\nexperience in data visualization\\r\\ncontributing to open source software projects (Github, ...)\\r\\nIf you believe you have a very strong profile in Data Science but are missing one of the must haves, don’t hesitate to send your application anyway.\""
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_jobPost['SovrenData']['SourceText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobSkill(jsonTaxanomy, Required = False):\n",
    "    skill = []\n",
    "    for i in range(0,len(jsonTaxanomy)):\n",
    "        for j in range(0,len(jsonTaxanomy['Subtaxonomy'][i]['Skill'])):\n",
    "            dictSkill = jsonTaxanomy['Subtaxonomy'][i]['Skill'][j]\n",
    "            if dictSkill[\"@existsInText\"]== True and dictSkill[\"@required\"]== Required:\n",
    "                skill.append(dictSkill['@name'])\n",
    "            if \"ChildSkill\" in  dictSkill.keys() and dictSkill[\"ChildSkill\"][0][\"@existsInText\"]== True and dictSkill[\"ChildSkill\"][0][\"@required\"]== Required:\n",
    "                skill.append(dictSkill[\"ChildSkill\"][0]['@name'])\n",
    "    return skill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectJobDescriptionSkill(jsonobject):\n",
    "\n",
    "    skillDict ={}\n",
    "    requiredSkill = []\n",
    "    desiredSkill = []\n",
    "    #extract information technology Skill taxanomy at the index 0\n",
    "    InformationTaxanomy = jsonobject['SovrenData']['SkillsTaxonomyOutput'][0]['Taxonomy'][0]\n",
    "    requiredSkill = getJobSkill(InformationTaxanomy, Required= True)\n",
    "    desiredSkill = getJobSkill(InformationTaxanomy, Required= False)\n",
    "    #extract engineering Skill taxanomy at the index 1\n",
    "    EngineeringTaxanomy = jsonobject['SovrenData']['SkillsTaxonomyOutput'][0]['Taxonomy'][1]\n",
    "    requiredSkill.extend(getJobSkill(EngineeringTaxanomy, Required= True))\n",
    "    desiredSkill.extend(getJobSkill(EngineeringTaxanomy, Required= False))\n",
    "\n",
    "    skillDict['requiredSkill']= requiredSkill\n",
    "    skillDict['desiredSkill']= desiredSkill\n",
    "    return skillDict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[': Phone Number |\\tEmail - id\\r']\n"
    }
   ],
   "source": [
    "# get the candidate  skills from \n",
    "\n",
    "dbCandidateSoveren = db['resumes_sovren']\n",
    "recent_jobPost = dbCandidateSoveren.find_one({ '_id': ObjectId('5e60f5895a90883323e38bc0') })\n",
    "# r = recent_jobPost['SovrenData']['SkillsTaxonomyOutput'][0]['Taxonomy']\n",
    "\n",
    "recent_jobPost['Resume']['StructuredXMLResume']['Qualifications']['QualificationSummary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CandidateSkillDescription(candidateDescription):\n",
    "    JobDescription = ''\n",
    "    count = len(recent_jobPost['Resume']['StructuredXMLResume']['EmploymentHistory']['EmployerOrg'])\n",
    "    for i in range(0,count):\n",
    "        JobDescription += recent_jobPost['Resume']['StructuredXMLResume']['EmploymentHistory']['EmployerOrg'][i]['PositionHistory'][0]['Description']\n",
    "    return JobDescription\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Responsibilities:\\r\\n* Experience in working with Computer Vision projects, building models using MobileNet, ImageNet classifiers using\\r\\nCaffe framework.\\r\\n* Experience in developing a live Face-Recognition model using Google\\'s Facenet.\\r\\n* Experience in developing image instance segmentation models using DeepLab and MaskRCNN for different kinds of problem statements.\\r\\n* Hands on experience in using Open CV for image data preparation. Experience in applying object detection models\\r\\nlike Mask R-CNN, YOLO, MobileNet.\\r\\n* Experience in working with grayscale and color images while preparing the datahttps://freesearch.naukri.com/preview/printResume?uname=082   44574508280a5400134f17425c4c6&sid=3810263223&AT=1580667059\\r\\n2/2/20 19\\'12\\r\\n\\r\\n\\r\\nResponsibilities:\\r\\n* Extensive experience in building robust Statistical Machine Learning and Deep Learning models to various business\\r\\nproblems and generating data visualizations using Python.\\r\\n* Hands-on expertise with all MachineLearning Models and concepts such as Classification, Regression, Clustering,\\r\\nEnsemble techniques.\\r\\n* Expertised in Data preparation, Integration, Mining, Manipulation, Exploration, Feature Engineering, developing\\r\\nModels and improving the efficiency and accuracy by evaluating the model.\\r\\n* Working experience with Python Data Visualization Libraries like Matplotlib, Seaborn, Plotly, ggplot, Altair, Geoplotlib\\r\\nfor plotting graphs like Scatter plots, Bar charts and Histograms, Line plots, pie charts, heat maps, time series, Box plots,\\r\\nDensity plots and more.\\r\\n* Extensively utilized NumPy, SciPy, Pandas, RDDs and DataFrames in performing preprocessing and exploratory data\\r\\nanalysis throughout the machine learning process. Efficient in writing and optimizing the code inPython.\\r\\n* Old hands on all python libraries used for data loading, feature extraction, data exploration, model building, evaluation\\r\\nand testing.\\r\\n* Experience on various applications using Python integrated IDEs such as Anaconda, Jupyter Notebook, Pyspark and Eclipse.\\r\\n* Hands-on Experience in various modern Programming and Query Languages like Python, JavaScript and experience\\r\\nin SQL, Hive.\\r\\n* Experience in building a product AML (Anti Money Laundering) using Machine Learning models, backend Django,\\r\\nfrontend HTML, CSS, Bootstrap with HIVE Database.Responsibilities:\\r\\n* Hands on experience in Data Analytics using Python, extensively utilized Pandas, NumPy.\\r\\n* Experience in building Machine Learning modelslike K-means Clustering, Linear Regression, Logistic Regression,\\r\\nDecision Trees, Random Forest on sample datasets.\\r\\n* Hands on all python libraries used for data loading, feature extraction, data exploration, model building, evaluation and testing.\\r\\n* Worked on deep learning and developed neural networks using TensorFlow in python.\\r\\n* Developed a CNN model for \"Captcha Recognition\"\\r\\n* Experience on various applications using Python IDE\\'s Anaconda, Jupyter Notebook.'"
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CandidateSkillDescription(recent_jobPost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'requiredSkill': ['DEEP LEARNING',\n  'PCA',\n  'MACHINE LEARNING',\n  'ELASTICSEARCH',\n  'TERABYTES',\n  'MACHINE LEARNING',\n  'PYTHON',\n  'NUMPY',\n  'DOCKER',\n  'ARCHITECTURE'],\n 'desiredSkill': ['DATA SCIENCE',\n  'DATA SCIENCE',\n  'DATA VISUALIZATION',\n  'SQL',\n  'DATABASE',\n  'VISUALIZATION',\n  'VISUALIZATION',\n  'VISUALIZATION']}"
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectJobDescriptionSkill(recent_jobPost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDescriptionsList(resumes):\n",
    "    desc = np.array([])\n",
    "    for res in resumes:\n",
    "        orgs = res['EmploymentHistory']['EmployerOrg']\n",
    "        for org in orgs:\n",
    "            roles = org['PositionHistory']\n",
    "            for role in roles:\n",
    "                desc = np.concatenate( (desc, [{\n",
    "                    'employer': org['EmployerOrgName'],\n",
    "                    'role': role['OrgName']['OrganizationName'],\n",
    "                    'description': role['Description'],\n",
    "                }]))\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([{'employer': 'Alstom Company', 'role': 'Alstom Company', 'description': \"OnBoardsystemofSCMTRailway forSoftwareIntegration\\r\\nClient:-Alstom Company\\r\\n?Validationoftechnicaldocumentation(Requirementsspecification, SoftwareArchitectureand DetailDesigndocuments, SafetyConcepts, TechnicalsafetyReports, TestSpecificationsand Reports, User manuals).\\r\\n?SoftwareVerification, ValidationandTestingofasoftwareproduct.\\r\\nAnalysisandexecutionofsystemtests.\\r\\nCheckingcompliancewiththerelevantstandards.(CENELECEN:   ~~~~~  ).\\r\\nPerformedbyFunctionalTesting.\\r\\nTest strategy, test system integration, and plan definition, write or review test specification, lead test execution, review and approve test reports.\\r\\nTo formalize anomalies if any, to organize review to analyze anomalies and to request modifications if required.\\r\\n?To carryout regression tests after anomalies correction.\\r\\nTestIntegrationPerformedinRTRTTool.\\r\\nProgramminginADALanguage.\\r\\n\\r\\nMaster's Degree Thesis Title is Software Integration Test for Railway Control Systems.\\r\\n\\r\\nDevelopment of GATEWAYTB HTTP SERVER PROTOCOL for 2nd GENERATION RADIO\\r\\nBLOCK CENTRE (RBC2G) ON C#.\\r\\nClient:-RFI Company (Protocol and related document is Ansaldo STS Company)\\r\\n1. ThenetworkprotocolusedbytheGatewayTBapplication(fromversion2.5.0.0)toexchange information with external\\r\\napplications about the simulated trains and the state of the line.\\r\\n2. The HTTP protocol is a request/response protocol. Once the TCP/IP connection is established, a client application\\r\\nsends a request to the server application (GatewayTB) a then waits for the response. In the request message, the client,\\r\\naccordingly to the HTTP protocol, sends the HTTP version it can manage; the behavior of the server depends on such\\r\\nversion: if the client requested HTTP 1.0 then the server, as soon as it has sent the response, closes the connection; if the client requested HTTP 1.1, the server, after sending the response, waits for a new request on the same connection until a\\r\\nconfigured timeout (since the last request) elapses or the client itself closes the connection.\\r\\n3.: thefirstpartis in HTML format allowing any\\r\\ninternet browser to show the data; the second part is in XML format and can be easily parsed by a personalized client\\r\\napplication. C# libraries to build parser.\\r\\n4. #Programming language in class library by\\r\\nusing Visual Studio 2017.\"},\n       {'employer': 'Alstom Company', 'role': 'Gleo Technology Pvt. Ltd', 'description': '1. The job role is SQL developer providing raw data for analysis purposes to the sales and marketing team of the company.\\r\\n2. Responsible for designing advanced SQL queries, procedure, cursor, triggers, batch files.\\r\\n3. PL-SQL (Triggers & Procedures) is written for updating the data into the database to update the reporting purpose\\r\\ntables.\\r\\n4. Updating prices, tax structure into the database on request for the sales operation team.'},\n       {'employer': 'PQE Group', 'role': 'PQE Group', 'description': ''},\n       {'employer': 'Accrosian Soft Solution Pvt. Ltd', 'role': 'Accrosian Soft Solution Pvt. Ltd', 'description': \"Collaborate with design and Front end and Back end programming teams to concept, build, test and launch dynamic websites per industry best practices.\\r\\nWork closely with other web developers to ensure the client's marketing goals and objectives are\\r\\nbeing understood and met within established timelines and with the highest level of quality.\\r\\nExpert with WordPress CMS.\\r\\nWorking in an E-Commerce Agile environment.\\r\\nCoding in a WordPress environment and develop and/or update code for themes and plugins.\\r\\nModify existing code as needed.\\r\\nCoding of custom WordPress theme and template files using HTML, CSS.\"},\n       {'employer': 'AKKA Technologies', 'role': 'AKKA Technologies', 'description': '4200130a6116050c500d5440564c6&sid=3810263223&AT=1580666203'},\n       {'employer': 'KJ Systems India Pvt ltd', 'role': 'KJ Systems India Pvt ltd', 'description': 'we used to maintain the Hospital data and perform operations on the data'},\n       {'employer': 'KJ Systems India Pvt ltd', 'role': 'kJ Systems India Pvt ltd', 'description': ''},\n       {'employer': 'Doctors', 'role': 'Doctors', 'description': 'Onsite / Offsite: Offsite'},\n       {'employer': 'WIPRO LIMITED as C', 'role': 'WIPRO LIMITED as C', 'description': \"Working as a software developer in WIPRO LIMITED. I am currently working as C/C++ developer with an expertise in Linux, Algorithms, Mysql, IPC, Linux programming, redis database, socket programming etc.\\r\\n\\r\\nI have alse been working in IOT technologies and have basic knowledge of java and Jenkins.\\r\\n\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     80809580f211d4e58485f0f1207006&sid=3810263223&AT=1580666275\\r\\n2/2/20 19'03\"},\n       {'employer': 'Infosys Limited', 'role': 'Infosys Limited', 'description': 'Responsibilities: Worked for a client in Database Administration and Production Operation role.   Domains: Individual & Family Services'},\n       {'employer': 'Online University Admission System', 'role': 'Online University Admission System', 'description': 'Onsite / Offsite: Onsite'},\n       {'employer': '', 'role': None, 'description': 'fresher as Fresher\\r\\nN/A to Till Date\\r\\ni am fresher 2019 pass out. i have post graduated in MCA'},\n       {'employer': 'Mavenir Systems', 'role': 'Mavenir Systems', 'description': \"IMS Conference Scheduler:\\r\\nI was involved in the implementation of conference scheduler for sdc platform.\\r\\nresponsibilities included\\r\\n1. Bug fixing and code cleanup.\\r\\n2. Writing scxml scripts for product's work flow and for rest requests.\\r\\n3. Dev side sanity testing, through automated test cases in python scripts with json\\r\\ndata.\\r\\n4. Debug errors analysing logs.\\r\\nTechnologies: c++, scxml, python(test scripts), Couchbase, json, Rest.\"},\n       {'employer': 'Brontobyte Analytics', 'role': 'Brontobyte Analytics', 'description': \"as Data Scientist\\r\\nJun 2018 to Aug 2019\\r\\nKey Result Areas:\\r\\nWorked on Data Science & Management Projects entailing collection of large & complex datasets and performing\\r\\nseveral statistical tests to obtain useful information from raw data\\r\\nAssisted clients on large scale data and analytics using advanced statistical and machine learning models\\r\\nArchitected and implemented analytics and visualization components for data analysis to predict relevant solutions\\r\\nCreated and presented the results of data analysis to show the trends using R Language & Python\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     e5100491779465e401804435a0e176&sid=3810263223&AT=1580666510\\r\\n2/2/20 19'05\\r\\n\\r\\n\\r\\nConsulted with Trained Analysts to effectively access data & optimize Tableau dashboards\\r\\nMaintained SQL scripts to create and populate tables in data warehouse for daily reporting across departments\\r\\nFacilitated functions such as Incident & Change Management, created solution prototype, participated in technology\\r\\nselection, controlled solution development and also supported project management activities\\r\\nReported overall status of the project to the client and senior management\"},\n       {'employer': 'Brontobyte Analytics', 'role': 'BRONTOBYTE ANALYTICS', 'description': 'Worked on collection of data sets, data mining, predictions, image processing related to data science in BRONTOBYTE\\r\\nANALYTICS'},\n       {'employer': 'analytics pvt ltd', 'role': 'analytics pvt ltd', 'description': 'Data collection, data pre-processing, visualization, drawing mathematics formula, data division, model utilization and accuracy calculations.'},\n       {'employer': 'analytics pvt ltd', 'role': 'analytics pvt ltd', 'description': 'Data collection, data pre-processing, drawing models, using algorithm, accuracy calculation of the model'},\n       {'employer': 'Dimenionless Techacademy', 'role': 'Dimenionless Techacademy', 'description': 'Working on the data sets for gaining insights into the datasets.\\r\\nProjects done-\\r\\nObjective-Car Price Prediction.\\r\\nAchievements-Performed the EDA on the datasets provided, dealt with the missing values, developed the predictive model with parameter tuning.\\r\\nKey skills-Python, Data Visualization, Linear Regression, Decision Tree.\\r\\nObjective- Exploratory Data Analysis on Titanic Data.\\r\\nAchievements-Performed the exploratory data analysis to understand the various insights using data visualization tools like matplotlib, seaborn.\\r\\nKey skills-Python, Data Visualization tools'},\n       {'employer': 'C&S Electric Ltd', 'role': 'C&S Electric Ltd', 'description': \"35846181c2d5e0c1b4a550f504e1c6&sid=3810263223&AT=1580666727\\r\\n2/2/20 19'06\\r\\n\\r\\n\\r\\nWorked in coordination with the sales and engineering team and prepared the competitor's data specifying the features of the different bus trunking ranges.\\r\\nStudying the customer's database and analyzing the prospective region of sales and the customer's response to C&S\\r\\nproducts as compared to competitor's products.\\r\\nInteracting with dealers and gathering information about the market trends.\\r\\nSkills acquired-market research, understanding customer requirements, the latest advancement in technology.\"},\n       {'employer': 'TechSavvy Engineers Pvt.Ltd', 'role': 'TechSavvy Engineers Pvt.Ltd', 'description': 'Creating the database of customers and bifurcating them as existing, potential and AMC(Annual Maintenance Contract)\\r\\ncustomer as per guidelines provided by Dassault Systems.\\r\\nAnalyzing the database and making insights about potential customers and arranging technical meetings on the basis of their requirements.\\r\\nPreparing quarterly reports of sales and visualizing the reports through pie charts, line charts, etc.'},\n       {'employer': 'KARVY ANALYTICS LIMITED', 'role': 'KARVY ANALYTICS LIMITED', 'description': 'Risk Assessment of Country-by-Country Reporting (CbCR)\\r\\nSchema was designed by studying CbCR schema guide and extracted data to csv from xml files for both inbound & outbound.\\r\\nLatest data was compared with old data to keep new & revised records and delete repeated records.\\r\\nPerformed deficiency analysis to find discrepancies in data & text analysis on additional info data to fetch multiplication factor.\\r\\nCalculated Indian footprint & made rules and applied to identify high risk jurisdictions and MNEs.\\r\\nTools & Techniques: Python, NumPy, Excel'},\n       {'employer': 'AI-ML-NLP LAB, Dept. Of Computer Science', 'role': 'AI-ML-NLP LAB, Dept. Of Computer Science', 'description': \"Developing MM-NAEMO: an multimodal many-objective optimization algorithm.\\r\\nIt is based on Genetic algorithm, developed to solve MMO problems from CEC2019 competition.\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     8151b1a0d61544c1001011702154e6&sid=3810263223&AT=1580666732\\r\\n2/2/20 19'06\\r\\n\\r\\n\\r\\nAchieved 5th position in accuracy using MM-NAEMO & published in IEEE Xplore as a paper.\\r\\nUsefull for Automatic text summarization, feature selection, numerical optimization etc.\\r\\nTools & Techniques: Python, Matlab, Niching, Selection, Mutation, Cross-over, GMM\"},\n       {'employer': '', 'role': None, 'description': 'Project: Ratabase Insurance rating\\r\\n\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     00319485e2d5e0c1b4a550f504e1c6&sid=3810263223&AT=1580666735'},\n       {'employer': 'Liberty Mutual, Hartford', 'role': 'Liberty Mutual, Hartford', 'description': 'Excel add-in etc.\\r\\nDomain: Insurance\\r\\nMethodology: Agile\\r\\nTools for Ticket: JIRA\\r\\n\\r\\nDescription: CGI Ratabase is a stand?alone insurance rating and product configuration solution designed to modernize and advance insurance rating. It empowers carriers to quickly deploy extremely complex rating programs and product\\r\\nsegmentations for a fraction of the cost of other solutions.\\r\\n\\r\\n\\r\\nTraining and Product Development:'},\n       {'employer': 'Liberty Mutual, Hartford', 'role': None, 'description': 'Product Development:\\r\\nPotentiometer: Build a reliable learning system which can eliminate the need of technical interviews. Technologies:\\r\\nAsp.Net Core, Angular 6, MVC, Micro services, Entity Framework, SQL Server.\\r\\nDeveloped complete back?end Micro services for the contributor and learner so that a person can learn technologies or\\r\\ncan contribute (if expert).\\r\\nDeveloped Front?end components for the application, collecting endpoints to retrieve data and displaying the data on the browser.'},\n       {'employer': '', 'role': None, 'description': 'Project: Ratabase Insurance rating\\r\\n\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     00319485e2d5e0c1b4a550f504e1c6&sid=3810263223&AT=1580666735'},\n       {'employer': 'Liberty Mutual, Hartford', 'role': 'Liberty Mutual, Hartford', 'description': 'Excel add-in etc.\\r\\nDomain: Insurance\\r\\nMethodology: Agile\\r\\nTools for Ticket: JIRA\\r\\n\\r\\nDescription: CGI Ratabase is a stand?alone insurance rating and product configuration solution designed to modernize and advance insurance rating. It empowers carriers to quickly deploy extremely complex rating programs and product\\r\\nsegmentations for a fraction of the cost of other solutions.\\r\\n\\r\\n\\r\\nTraining and Product Development:'},\n       {'employer': 'Liberty Mutual, Hartford', 'role': None, 'description': 'Product Development:\\r\\nPotentiometer: Build a reliable learning system which can eliminate the need of technical interviews. Technologies:\\r\\nAsp.Net Core, Angular 6, MVC, Micro services, Entity Framework, SQL Server.\\r\\nDeveloped complete back?end Micro services for the contributor and learner so that a person can learn technologies or\\r\\ncan contribute (if expert).\\r\\nDeveloped Front?end components for the application, collecting endpoints to retrieve data and displaying the data on the browser.'},\n       {'employer': 'Tata Consultancy Services', 'role': 'Tata Consultancy Services', 'description': \"Key Result Areas:\\r\\n\\r\\nWorking with:\\r\\no\\tQlik Sense analytical tool\\r\\n\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     b0e5c0c504a1a011705134446590a6&sid=3810263223&AT=1580666739\\r\\n2/2/20 19'07\\r\\n\\r\\n\\r\\no\\tSQL to fetch data from master table & merge in another table, to fetch data from different tables on certain conditions\\r\\nMaintaining the track of certain jobs to be completed by person\\r\\nAddressing & resolving issue if any order is not progressing & contact certain person for its progression\\r\\n\\r\\nAssisting in creating policies & procedures regarding collating & analyzing data; developing new data analysis\\r\\nprocesses\\r\\nCritically evaluating and screening data, plus profiling to identify any issues; assessing analytics on an ongoing basis\\r\\nMonitoring the data warehouse, including the collection & utilization of all data\\r\\nExtending knowledge of Business Intelligence to provide ongoing refinement of processes to improve operations\\r\\nReviewing & suggesting suitable improvements in data models; enforcing data modelling standards, best practices in the design\\r\\n\\r\\nSignificant Achievements:\\r\\nPrepared reports based on research conducted and made important suggestions on investments for companies as well as individuals\\r\\nProvided support through market analysis, forecasting, and reporting to the aligned internal business unit\"},\n       {'employer': 'Tata Consultancy Services', 'role': 'Tata Consultancy Services', 'description': ''},\n       {'employer': 'Fractal Analytics', 'role': 'Fractal Analytics', 'description': 'Data science, machine learning, data visualisation, python etc'},\n       {'employer': 'HSBC Global Banking and Markets', 'role': 'HSBC Global Banking and Markets', 'description': ''},\n       {'employer': 'HSBC Global Banking and Markets', 'role': 'SME', 'description': 'Financial Regulations, Development, OTC Derivatives\\r\\nDeveloped and deployed a firm wide BCBS239 compliant Qlikview dashboard to track the number of data quality issues in\\r\\nOTC Derivative trades (3 to 4 million trades on a daily basis) across different asset classes (FX, IR, CR, CO and EQ) and regions (HSBC EU, HSBC US   )\\r\\nDesign, Develop and Deploy the data quality ETL(Extract, Transform and Load) pipelines in Data360Analyze(Advanced analytics Software) for different asset classes and different financial regulations (DFA, EMIR, HKMA and MIFID) to perform\\r\\nreconciliations and data quality checks for accurate reporting\\r\\nTest and validate the results to deploy the controls on the Data360Analyze(Advanced analytics Software) production\\r\\nserver\\r\\n\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     d48181c0079061742504d1d421e056&sid=3810263223&AT=1580666743'},\n       {'employer': 'HSBC Global Banking and Markets', 'role': 'HSBC Global Banking And Markets', 'description': ''},\n       {'employer': 'HSBC Global Banking and Markets', 'role': 'Trade Data Quality HSBC Global Banking and Markets', 'description': 'Automated the manual process of extracting trade data statistics from Qlikview Dashboard to Excel Worksheets. The\\r\\nautomation has saved an FTE every day\\r\\nDeveloped a Qlikview dashboard to capture the production capacity of different teams across Global Markets Middle\\r\\nOffice\\r\\nDeployed the dashboard on the server to generate monthly production stats for higher management\\r\\nDeveloped and deployed a Qlikview dashboard for the Chief Data Office to track the data quality statistics across different\\r\\nregions in HSBC'},\n       {'employer': 'HSBC London', 'role': 'HSBC London', 'description': 'Trained and mentored the analysts on execution, deployment and development of controls on production'},\n       {'employer': 'ITC Infotech', 'role': 'ITC Infotech', 'description': 'Joined as a part of team handling predictive maintenance\\r\\n   ~~~~~~~~~~~~   was given on how to handle the various\\r\\nphases of a project including analyzing the data, getting\\r\\ninsights from data, finding relation between variables in data, data preparation, exploratory analysis, feature\\r\\nengineering, model selection, model training'},\n       {'employer': 'Invicta Group', 'role': 'Invicta Group', 'description': \"Procurement engineer in one of the leading fire protection\\r\\ncompany in middle east.Responsibilities include market\\r\\nanalyzing, estimation for new projects, material forecasting, material sourcing from local and international\\r\\nmarket, tender evaluation, material purchasing, periodic\\r\\n\\r\\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     e01554c1379465e401804435a0e176&sid=3810263223&AT=1580666745\\r\\n2/2/20 19'07\\r\\n\\r\\n\\r\\nsupplier evaluation etc\\r\\n\\r\\n\\r\\nITC Infotech as Data Science Intern\\r\\nN/A to Till Date\"},\n       {'employer': '', 'role': None, 'description': 'Have worked at Nielsen as Data Analyst for Projects like Coca-Cola, Nestle etc. Mainly working on R Programming, SQL,\\r\\nRapid Miner, Advance Excel, Data Mining, Regression etc.'},\n       {'employer': 'Maharaja Sayajirao University', 'role': 'Maharaja Sayajirao University', 'description': 'To deliver Statistics lectures and take respective laboratories.\\r\\nTo design and assess test papers.\\r\\nDo supervisions in exam as directed by department of Statistics.'}],\n      dtype=object)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildDescriptionsList(db_resumes.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}