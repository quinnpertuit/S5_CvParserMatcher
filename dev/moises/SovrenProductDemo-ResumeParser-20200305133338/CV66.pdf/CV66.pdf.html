<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<style>
	p.std   { margin-top: 0; margin-bottom: 0; border: 0 0 0 0; }
</style>
</HEAD>
<BODY>

<!-- [[[ PDF.Page-->
<BR>
2/2/20 23&amp;09<BR>
Página 1 de 3 https://freesearch.naukri.com/preview/printResume?uname=ae3…849521844280a5400134f17425c4c6&amp;sid=3810263223&amp;AT=1580681122<BR>
Current Designation: java developer<BR>
Current Company: ZS Associates<BR>
Current Location: Pune<BR>
Pref. Location: Pune<BR>
Functional Area: IT Software - DBA / Datawarehousing<BR>
Role: Software Developer<BR>
Industry: IT-Software/Software Services<BR>
Marital Status: Single/unmarried<BR>
Total Experience: 2 Year(s) 0 Month(s)<BR>
Notice Period: Currently Serving Notice Period<BR>
Highest Degree: B.Tech/B.E. [Instrumentation]<BR>
Verified : &nbsp;&nbsp;<BR>
Phone Number | <BR>
&nbsp;<BR>
Email - id<BR>
 &nbsp;ZS Associates as java developer<BR>
Sep 2019 to Till Date<BR>
Project Undertaken<BR>
1) Project : Revo (Internal Project)<BR>
Customer : Borehinger Illenghim (Pharmaceutical Domain)<BR>
Duration : September 2019 ? November 2019<BR>
Description : Migration of Data from data sources like Iqvia, Veeva, Bitman, OneCustomer, NewTrans, Integrichain, PlanIT,<BR>
to Hadoop using different frameworks like Azkaban Work Designer, Cloudberry, AWS services like S3 bucket , AWS Glue<BR>
and data refresh on Tableau (HQ Fields) and MSTR Fields .<BR>
Languages/Tools : EMR Cluster, CloudBerry, FileZilla, SQLWorkBench, ETL Tools like RDM , Azkaban Work Designer,<BR>
AWS Services like S3 Bucket, AWS Glue.<BR>
Roles and Responsibilities:<BR>
Actively participated in interaction with Technical Lead to fully understand the requirements of the project.<BR>
Worked with s3 bucket to fetch the data for the data sources and oad it through Azkaban Work Designer.<BR>
Used Agile methodology for all the development activities.<BR>
ID: 1197477a584b09df30 Last Active: 3-Feb-20 Last Modified: 1-Feb-20<BR>
 <BR>
Ayusi Saha &nbsp;<BR>
&nbsp;<BR>
Service Delivery Analyst seeking roles in Database ingestion,Data Warehousing,Data<BR>
Modeling,Data Extraction,Business Intelligence,Performance<BR>
Tuning,,Hive,Sqoop,Oracle,BitBucket.Also worked as java developer in Core<BR>
Java,Kaka,Mongodb<BR>
 <BR>
&nbsp;<BR>
&nbsp;<BR>
Key Skills: Service Delivery Analyst,Software Developer,Database Administration,Data Warehousing,Data<BR>
Architecture,Data Modeling,Data Extraction,Business Intelligence,Performance Tuning,Core<BR>
Java,Kaka,MondoDb,NOSQL,Hive,Sqoop,Oracle,BitBucket,ETL Development<BR>
 <BR>
&nbsp;<BR>
&nbsp;<BR>
&nbsp;<BR>
Summary<BR>
Result-oriented Professional with 2 years of experience as hadoop developer in Database ingestion ,Data Warehousing,Data<BR>
Architecture,Data Modeling,Data Extraction,Business Intelligence,Performance Tuning, ,Hive, Sqoop,Oracle,BitBucket.Also<BR>
worked as java developer in Core Java,Kaka,MondoDb,NOSQL. <BR>
&nbsp;<BR>
Work Experience<P class="std" >&nbsp;</P>
<BR>
<BR>

<!-- ]]] PDF.Page-->

<!-- [[Page]] -->
<P style="page-break-before:always; border-top-style: dashed; border-top-width:thin; color:silver; " ></P>
<!-- [[[ PDF.Page-->
<BR>
2/2/20 23&amp;09<BR>
Página 2 de 3 https://freesearch.naukri.com/preview/printResume?uname=ae3…849521844280a5400134f17425c4c6&amp;sid=3810263223&amp;AT=1580681122<BR>
 &nbsp;TATA Consultancy Services as Java Developer &amp; Hadoop Developer<BR>
Mar 2018 to Sep 2019<BR>
Project Undertaken<BR>
1) Project : BIAN(Wave-1) and BIAN(Wave-2)<BR>
Customer : PNC Financial Services (Banking And Finance Domain)<BR>
Duration : July 2018 - Sep 2018<BR>
Description : In this,I have created the REST API header by using JSON Framework.RESTful applications use HTTP<BR>
requests to POST (create), PUT (create and/or update), GET (e.g., make queries), and DELETE data. REST uses HTTP<BR>
for all four CRUD<BR>
(Create/Read/Update/Delete) operations and also uses ISO LINK.<BR>
Roles &amp; Responsibilites : Developing program as per client requirement.<BR>
Unit testing of the whole project.<BR>
Enhancement and Updation with new task assigned.<BR>
Languages/Tools : SwaggerHub- SmartBear<BR>
2) Project : NOSQL MVP1<BR>
Customer : PNC Financial Services (Banking And Finance Domain) Duration : Sep 2018 - April 2019<BR>
Description : Migration of Data from Traditional Mainframe Systems to NOSQL(MongoDb) Database.<BR>
Helps in Development of Producer and Consumer Code in Java using Kafka Technology for DataStreaming.<BR>
Roles &amp; Responsibilites : Developing and enhancing the existing program as per client requirement.<BR>
Unit testing of the whole project.<BR>
Error resolution and discrepancies in existing program.<BR>
Languages/Tools : IRAD, Java Maven project, Putty, WinSCP, Microservices, kafka,NOSQL(Mongodb)<BR>
3) Project : BDH (Big Data Hadoop)<BR>
Customer : PNC Financial Services (Banking And Finance Domain) Duration : May 2019 - September 2019<BR>
Description : Migration of Data from Bigdata to Hadoop using different frameworks like impala, Sqoop, hive using<BR>
ingestions technique and Python Framework.<BR>
Roles &amp; Responsibilities : Pulling data from source side to HDFS using database ingestion, mainframe ingestion &amp; file<BR>
ingestion.<BR>
Unit testing of the whole project Trouleshooting the Error.<BR>
Languages/Tools : Winscp, putty, Hue frontend, CA7 Frontend<BR>
Roles and Responsibilities:<BR>
Actively participated in interaction with onsite team to fully understand the requirements of the object. ? Involved in the<BR>
documentation and development activities ? Used GIT for the version control during development.<BR>
Worked in three implementation projects.<BR>
Understood the proper flow of the project.<BR>
UG: B.Tech/B.E. (Instrumentation) from Institute of Technical Education and Research, Bhuvaneshwar in 2017<BR>
 <BR>
&nbsp;<BR>
&nbsp;<BR>
Education<BR>
 <BR>
&nbsp;<BR>
&nbsp;<BR>
IT Skills<BR>
Skill Name Version Last Used Experience<BR>
IRAD, Putty, WinSCP<BR>
FileZilla, SQLWorkBench<BR>
Core Java, Kaka<BR>
MondoDB<BR>
<BR>

<!-- ]]] PDF.Page-->

<!-- [[Page]] -->
<P style="page-break-before:always; border-top-style: dashed; border-top-width:thin; color:silver; " ></P>
<!-- [[[ PDF.Page-->
<BR>
2/2/20 23&amp;09<BR>
Página 3 de 3 https://freesearch.naukri.com/preview/printResume?uname=ae3…849521844280a5400134f17425c4c6&amp;sid=3810263223&amp;AT=1580681122<BR>
Affirmative Action<BR>
Physically Challenged: No<BR>
Work Authorization<BR>
Job Type: Permanent<BR>
Employment Status: Full time<BR>
Hive, Sqoop, Big Data<BR>
Oracle<BR>
Git, Bitbucket<BR>
Languages Known<BR>
Language Proficiency Read Write Speak<BR>
English &nbsp;&nbsp;<BR>
Hindi &nbsp;&nbsp;<BR>
&nbsp;<BR>
<BR>

<!-- ]]] PDF.Page-->

<!-- [[Page]] -->
<P style="page-break-before:always; border-top-style: dashed; border-top-width:thin; color:silver; " ></P></BODY></HTML>